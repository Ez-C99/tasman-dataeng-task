# Ezra Chamba Tasman Data Engineering Task ‚Äî USAJOBS ETL

End-to-end pipeline that pulls jobs from the USAJOBS API, persists raw pages to **S3 Bronze**, validates the data with **Great Expectations**, normalises to DTOs, and **upserts** into **Postgres**. Designed for local dev with Docker/Make, and cloud scheduling with **EventBridge ‚Üí ECS RunTask** (Terraform).

---

## ‚ú® Features

* **Bronze capture (S3):** gzipped JSON, deterministic bytes, SHA-256 metadata.
* **Data Quality gate (Great Expectations):** URL regex, non-nulls, pay range sanity, ‚Äú‚â•1 location‚Äù guard.
* **Idempotent loader (Postgres):** `INSERT ‚Ä¶ ON CONFLICT DO UPDATE` on natural key.
* **Orchestration:** `ingest_search_page()` wires HTTP ‚Üí Bronze ‚Üí GX ‚Üí Transform ‚Üí Load.
* **Infra-as-code:** ECS + ECR + EventBridge + IAM + CloudWatch Logs (Terraform, eu-west-2).
* **Developer ergonomics:** Makefile targets, Ruff/Mypy, unit + integration + smoke tests.

---

## üìö Project Docs (read this first)

These three docs are the backbone of the solution. They explain *why* each design choice was made, *how* the system evolved, and *what* was implemented‚Äîso reviewers can follow the journey, not just the code.

* **Task Brief ‚Äî annotated requirements (Goodnotes)**  
  üëâ [docs/Brief_Insights_and_Requirements.pdf](docs/Brief_Insights_and_Requirements.pdf)  
  The original task brief marked up by hand with notes. Captures the problem framing, constraints, and early insights that shaped priorities, acceptance criteria, and scope

* **Design Doc ‚Äî the ‚Äúwhy‚Äù & architecture**  
  üëâ [DESIGN_DOC.md](DESIGN_DOC.md)  
  Scope, constraints, target personas, and end-to-end architecture. Includes trade-offs, data model rationale, paging/rate-limits, medallion layers, and scheduling strategy. Use this to understand the north star before diving into code.  
  Related: Architectural Decision Records (ADRs) in [`docs/architecture/decisions/`](docs/architecture/decisions/index.md).

* **Implementation Doc ‚Äî the ‚Äúhow‚Äù & what shipped**  
  üëâ [IMPLEMENTATION_DOC.md](IMPLEMENTATION_DOC.md)  
  Feature-grouped narrative from Bronze ‚Üí Transform ‚Üí Loader ‚Üí DQ ‚Üí Scheduling. Each section lists tasks, acceptance criteria, risks/trade-offs, and notes. It ties commits/PRs to the plan and calls out important deltas and gotchas encountered during build.

* **Dev Log ‚Äî the daily story & context**  
  üëâ [DEV_LOG.md](DEV_LOG.md)  
  A chronological diary capturing decisions, detours, and timeboxing. This is helpful for interviewers and maintainers to see priorities, pressure points, and how the approach adapted under constraints.

**How to review (suggested flow):**

1) Skim the **Design Doc** for intent and architecture.  
2) Read the **Implementation Doc** sections 1‚Äì7 to see how the intent became code.  
3) Scan the **Dev Log** to understand sequencing and trade-off timing.  
4) Jump back here to acquaint youself with and run the project locally or deploy to ECS.

---

## üó∫Ô∏è Repo at a glance

```plaintext
tasman-dataeng-task
‚îú‚îÄ .dockerignore
‚îú‚îÄ .pre-commit-config.yaml
‚îú‚îÄ DESIGN_DOC.md
‚îú‚îÄ DEV_LOG.md
‚îú‚îÄ IMPLEMENTATION_DOC.md
‚îú‚îÄ LICENSE
‚îú‚îÄ Makefile
‚îú‚îÄ README.md
‚îú‚îÄ docker
‚îÇ  ‚îú‚îÄ Dockerfile
‚îÇ  ‚îî‚îÄ docker-compose.yml
‚îú‚îÄ docs
‚îÇ  ‚îú‚îÄ Brief_Insights_and_Requirements.pdf
‚îÇ  ‚îî‚îÄ architecture
‚îÇ     ‚îî‚îÄ decisions
‚îÇ        ‚îú‚îÄ 0001-runtime-extractor.md
‚îÇ        ‚îú‚îÄ 0002-scheduling.md
‚îÇ        ‚îú‚îÄ 0003-database.md
‚îÇ        ‚îú‚îÄ 0004-data-model-shape.md
‚îÇ        ‚îú‚îÄ 0005-idempotency-key-upsert.md
‚îÇ        ‚îú‚îÄ 0006-secrets-management.md
‚îÇ        ‚îú‚îÄ 0007-api-paging-and-limits.md
‚îÇ        ‚îú‚îÄ 0008-db-security-and-durability.md
‚îÇ        ‚îú‚îÄ 0009-data-quality.md
‚îÇ        ‚îú‚îÄ 0010-integration-testing.md
‚îÇ        ‚îî‚îÄ index.md
‚îú‚îÄ infra
‚îÇ  ‚îî‚îÄ terraform
‚îÇ     ‚îú‚îÄ .terraform.lock.hcl
‚îÇ     ‚îú‚îÄ iam_task.tf
‚îÇ     ‚îú‚îÄ main.tf
‚îÇ     ‚îú‚îÄ outputs.tf
‚îÇ     ‚îú‚îÄ providers.tf
‚îÇ     ‚îú‚îÄ s3_bronze.tf
‚îÇ     ‚îú‚îÄ secrets.tf
‚îÇ     ‚îî‚îÄ variables.tf
‚îú‚îÄ pyproject.toml
‚îú‚îÄ scripts
‚îú‚îÄ src
‚îÇ  ‚îî‚îÄ tasman_etl
‚îÇ     ‚îú‚îÄ __init__.py
‚îÇ     ‚îú‚îÄ config.py
‚îÇ     ‚îú‚îÄ db
‚îÇ     ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ     ‚îÇ  ‚îú‚îÄ engine.py
‚îÇ     ‚îÇ  ‚îú‚îÄ migrations
‚îÇ     ‚îÇ  ‚îÇ  ‚îú‚îÄ 001_init.sql
‚îÇ     ‚îÇ  ‚îÇ  ‚îî‚îÄ 002_child_timestamps.sql
‚îÇ     ‚îÇ  ‚îî‚îÄ repository.py
‚îÇ     ‚îú‚îÄ dq
‚îÇ     ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ     ‚îÇ  ‚îî‚îÄ gx
‚îÇ     ‚îÇ     ‚îî‚îÄ validate.py
‚îÇ     ‚îú‚îÄ http
‚îÇ     ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ     ‚îÇ  ‚îú‚îÄ codelists.py
‚îÇ     ‚îÇ  ‚îî‚îÄ usajobs.py
‚îÇ     ‚îú‚îÄ logging_setup.py
‚îÇ     ‚îú‚îÄ models.py
‚îÇ     ‚îú‚îÄ runner
‚îÇ     ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ     ‚îÇ  ‚îî‚îÄ run.py
‚îÇ     ‚îú‚îÄ storage
‚îÇ     ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ     ‚îÇ  ‚îî‚îÄ bronze_s3.py
‚îÇ     ‚îî‚îÄ transform.py
‚îî‚îÄ tests
   ‚îú‚îÄ conftest.py
   ‚îú‚îÄ integration
   ‚îÇ  ‚îú‚îÄ test_run_ingest_integration.py
   ‚îÇ  ‚îî‚îÄ test_upsert.py
   ‚îú‚îÄ smoke
   ‚îÇ  ‚îî‚îÄ test_dq_smoke.py
   ‚îî‚îÄ unit
      ‚îú‚îÄ test_bronze_s3.py
      ‚îú‚îÄ test_models.py
      ‚îú‚îÄ test_run_ingest.py
      ‚îú‚îÄ test_transform.py
      ‚îî‚îÄ test_validate.py

```

---

## ‚öôÔ∏è Prerequisites

* **Python 3.12**
* **Docker** (and `docker compose`)
* **Postgres** (local via compose)
* **AWS CLI v2** (for ECR/ECS/Logs), an AWS account, and a profile (e.g. `tasman-dev`)
* (For cloud) **Terraform** ‚â• 1.6

---

## üîê Configuration

Create a local `.env` (example below). In cloud, secrets are pulled from **AWS Secrets Manager**.

```env
# USAJOBS API
USAJOBS_USER_AGENT=your-registered-email@example.com
USAJOBS_AUTH_KEY=xxxxxxxxxxxxxxxxxxxx

# Bronze S3 (local runs may stub this; ECS requires real bucket)
BRONZE_S3_BUCKET=dev-tasman-task-usajobs
BRONZE_S3_PREFIX=bronze/usajobs

# Postgres
DB_URL=postgresql://postgres:localpw@localhost:5432/usajobs

# DQ gate
DQ_ENFORCE=true

# Default search params (used by orchestration)
KEYWORD=data
LOCATION_NAME=Chicago
MAX_PAGES=1
```

> In ECS, **Secrets Manager** names are configurable via Terraform variables:
>
> * `usajobs_auth_secret_name` (default `tasman/dev/usajobs/auth`)
> * `db_url_secret_name` (default `tasman/dev/db/url`)

---

## ‚ñ∂Ô∏è Local development

### Install & tooling

```bash
python -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"
pre-commit install
```

### Start services & run tests

```bash
# Start Postgres (and optional local etl container)
make up

# Apply DB migrations (idempotent)
make db-migrate

# Lint, type-check, and tests (unit + integration)
make test

# DQ / GX smoke suite with visible output
make dq        # or: make smoke

# Shutdown
make down
```

### One-off ETL run (local Python)

```bash
python -c "from tasman_etl.runner.run import ingest_search_page; \
print(ingest_search_page(run_id='local-test', page=1, keyword='data', location_name='Chicago, Illinois', radius_miles=None))"
```

---

## üß™ Data Quality (GE)

* Uses **Great Expectations** against a pandas DataFrame constructed from DTOs.
* Expectations include:

  * `position_id` / `position_title` non-nulls
  * `position_uri` matches `^https?://`
  * `pay_min` / `pay_max` ‚â• 0
  * `pay_min` ‚â§ `pay_max`
  * Python-side ‚Äúat least one location‚Äù per page
* **Note:** current implementation targets **pandas < 2.0** (see `pyproject.toml` pin).

Run the smoke test:

```bash
make dq
# or
pytest -s tests/smoke/test_dq_smoke.py
```

---

## üóÉÔ∏è Database (Silver)

Apply DDL:

```bash
psql "$DB_URL" -f src/tasman_etl/db/migrations/001_init.sql
```

Tables:

* `job(position_id unique, ‚Ä¶, raw_json jsonb)`
* `job_location(job_id, loc_idx unique per job)`
* `job_category(job_id, code unique per job)`
* `job_grade(job_id, code unique per job)`
* `job_details(job_id unique)`

Upserts are idempotent (`ON CONFLICT ‚Ä¶ DO UPDATE`). `raw_json` is stored as JSONB for reach-back.

---

## ü™£ Bronze layout (S3)

```bash
s3://<bucket>/<prefix>/date=YYYY/MM/DD/run=<run_id>/page=NNNN.json.gz
```

* **Deterministic gzip** (mtime=0) and SHA-256 checksum stored in object metadata.
* Bronze envelope contains `request`, `response` (headers + payload), and `ingest` metadata.

---

## ‚òÅÔ∏è Cloud deployment (ECS + EventBridge)

> Region assumed: **eu-west-2**. Adjust variables as needed.

### 1) Provision with Terraform

```bash
cd infra/terraform
terraform init -upgrade

# Provide your USAJOBS user agent on first apply
terraform apply \
  -var="aws_profile=tasman-dev" \
  -var="aws_region=eu-west-2" \
  -var="usajobs_user_agent=your-registered-email@example.com"
```

**Important TF variables (selection):**

* `usajobs_auth_secret_name` (or set `usajobs_auth_key` to have TF create one in dev)
* `db_url_secret_name` (preferred) or `db_url` (dev only)
* `schedule_expression` (default daily: `cron(0 0 * * ? *)`)
* `keyword`, `location_name`, `max_pages`
* `container_cpu`, `container_memory`

Outputs include:

* `ecr_repo_url`
* `ecs_cluster_arn`
* `task_definition_arn`
* `event_rule_name`
* `log_group`

### 2) Build & push image to ECR

```bash
export AWS_REGION=eu-west-2
export AWS_PROFILE=tasman-dev
REPO_URL=$(terraform -chdir=infra/terraform output -raw ecr_repo_url)
REGISTRY="${REPO_URL%%/*}"

aws ecr get-login-password --region "$AWS_REGION" --profile "$AWS_PROFILE" \
  | docker login --username AWS --password-stdin "$REGISTRY"

GIT_SHA=$(git rev-parse --short HEAD)
docker buildx build --platform linux/amd64 -t etl-temp:latest -f docker/Dockerfile --load .
docker tag etl-temp:latest "${REPO_URL}:latest"
docker tag etl-temp:latest "${REPO_URL}:${GIT_SHA}"
docker push "${REPO_URL}:latest"
docker push "${REPO_URL}:${GIT_SHA}"
```

> **Tip:** Prefer immutable tags (`:${GIT_SHA}`) and update the task definition to pin that digest for reproducible deploys. With `:latest`, consider re-registering the task definition on changes.

### 3) Trigger a manual run & tail logs

```bash
CLUSTER_ARN=$(terraform -chdir=infra/terraform output -raw ecs_cluster_arn)
LATEST_TD=$(terraform -chdir=infra/terraform output -raw task_definition_arn)

aws ecs run-task \
  --cluster "$CLUSTER_ARN" \
  --launch-type FARGATE \
  --task-definition "$LATEST_TD" \
  --network-configuration "awsvpcConfiguration={subnets=$(aws ec2 describe-subnets --filters Name=default-for-az,Values=true --query 'Subnets[0].SubnetId' --output text --region $AWS_REGION),assignPublicIp=ENABLED,securityGroups=$(aws ec2 describe-security-groups --filters Name=group-name,Values=$(basename $(terraform -chdir=infra/terraform output -raw ecs_cluster_arn) | sed 's/-cluster/-etl-sg/'))}" \
  --region "$AWS_REGION" --profile "$AWS_PROFILE"

# Logs
LOG_GROUP=$(terraform -chdir=infra/terraform output -raw log_group)
aws logs tail "$LOG_GROUP" --follow --region "$AWS_REGION" --profile "$AWS_PROFILE"
```

EventBridge will invoke the same **RunTask** on the configured schedule (UTC).

---

## üß∞ Make targets (quick reference)

```bash
make fmt           # Ruff import order + format
make lint          # Ruff checks
make type          # mypy
make db-migrate    # apply SQL migrations
make unit          # unit tests
make integration   # integration tests (depends on db-migrate)
make test          # db-migrate + lint + type + unit + integration
make dq            # GE smoke test (visible output)
make smoke         # run all smoke tests
make up            # docker compose up (db + local etl if configured)
make down          # docker compose down -v
```

---

## üß≠ Troubleshooting

* **ECR push: ‚Äúrepository ‚Ä¶ doesn‚Äôt exist‚Äù**
  Make sure `REPO_URL` is from TF output and not truncated; if you see `‚Ä¶-etl**atest**`, your tag string got mangled‚Äîre-tag then push.

* **ClusterNotFound on `run-task`:**
  Export `CLUSTER_ARN` from TF output; ensure `AWS_PROFILE`/`AWS_REGION` are set.

* **No logs:**
  Confirm log group name from TF output and tail with `aws logs tail`.

* **GE/pandas versioning:**
  The current GE harness targets **pandas < 2.0** (pinned). Upgrading pandas will require adapting `validate.py`.

---

## üìÑ License

MIT (see `LICENSE`).

---

## üß© What‚Äôs next

* Pin immutable image tag in task definition (Terraform update).
* Private networking hardening; RDS + SSL enforcement; metrics/alarms.
* CI: lint/type/test ‚Üí build ‚Üí push ECR ‚Üí Terraform plan/apply.
* Gold views & serving; performance tuning (bounded concurrency).
